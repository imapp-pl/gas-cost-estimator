---
title: "R Notebook: exploration of validation by comparing individual and total measurements; for Gas Cost Estimator"
output: html_notebook
---

```{r include=FALSE, fig.width=20}
library(sqldf)
```

## Data preparations

Read in the `programs.csv` as generated by `src/program_generator.py` with `--fullCSV` flag on.

We're using the random arithmetic program generator via: `python3 program_generator/pg_arythmetic.py generate --count=10 --opsLimit=10 --fullCsv > ../../local/pg_arythmetic_10_10.csv`.

Then tracing it using: `docker run   --rm -v /home/user/sources/imapp/local:/srv/local  -it gas-cost-estimator/geth_all sh -c "cd src && cat /srv/local/pg_arythmetic_10_10.csv | python3 instrumentation_measurement/measurements.py measure --evm geth --mode trace --sampleSize 1 > /srv/local/trace_pg_arythmetic_10_10.csv"`.

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")
programs = read.csv("../../local/pg_arythmetic_c100_ops10.csv")
traces = read.csv("../../local/trace_pg_arythmetic_c100_ops10.csv")
head(traces)
```
```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")
total_measurements = read.csv("../../local/geth_pg_arythmetic_c100_ops10.csv")
head(total_measurements)
```

```{r}
# temporary workaround for obtaining the estimated cost weights
setwd("~/sources/imapp/gas-cost-estimator/src")
estimated_cost = read.csv("../../local/rough_opcode_costs_new.csv")
head(estimated_cost)
```

```{r}
ops_per_program = data.frame(table(traces$program_id, traces$op, dnn=c("program_id", "op")))

estimated_programs = sqldf("SELECT program_id, sum(ops_per_program.Freq * estimated_cost.min) as cost
                           FROM ops_per_program
                           INNER JOIN estimated_cost ON ops_per_program.op == estimated_cost.op
                           GROUP BY program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg(total_measurements.measure_total_time_ns) as avg_measure_total_time_ns,
                      cost
                    FROM estimated_programs
                    INNER JOIN total_measurements ON total_measurements.program_id == estimated_programs.program_id
                    GROUP BY estimated_programs.program_id
                    ORDER BY cast(estimated_programs.program_id AS int)")

head(validation)
```

We need an additional simple (plottable) "fingerprint" of the OPCODEs included in the program, to evaluate the validation plot at a glance.

```{r}
library(nlme)
fingerprint <- function(x) {
  x[which(!(x$op %in% c('PUSH32', 'PUSH1', 'STOP'))),'op']
}
has_exp <- function(x) {
  if ('EXP' %in% x$op) 'red' else 'blue'
}
count_zero_args <- function(x) {
  sum(!is.na(x$stack_elem0) & x$stack_elem0 == 0 | !is.na(x$stack_elem1) & x$stack_elem1 == 0 | !is.na(x$stack_elem2) & x$stack_elem2 == 0)
}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'stack_elem0', 'stack_elem1', 'stack_elem2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

Basic inspection of data reveals strong decreasing trend for each sample (sequence of runs) and a lot of outliers.

```{r}
plot(total_measurements$measure_total_time_ns, ylim=c(0,3200))
boxplot(total_measurements$measure_total_time_ns)
boxplot(measure_total_time_ns ~ program_id, data=total_measurements, outline=FALSE)
# tapply(total_measurements$measure_total_time_ns, total_measurements$program_id, hist, probability=T)
# tapply(total_measurements$measure_total_time_ns, total_measurements$program_id, summary)
```

The relation between the estimated `cost` and the measured `avg_measure_total_time_ns` is quite evident for
"cheap" programs, but not anymore for "expensive" program in the set.
Educated guess was it is because of `EXP`, which has been measured individually with smallest argument in the Stage I, and those measurements led to the estimated `cost`.
Thereby the `EXP` programs are consistently underestimated.

```{r}
plot_validation <- function(df) {
  scatter.smooth(df$cost, df$avg_measure_total_time_ns, col=df$has_exp)
  text(avg_measure_total_time_ns~cost, data=df, labels=program_id, cex=0.6, pos=4, font=2)
  text(avg_measure_total_time_ns~cost, data=df, labels=n_zero_args, cex=0.6, pos=1, font=2)
}
plot_validation(validation)
```

```{r}
lm_validation <- function(df) {
  summary(lm(avg_measure_total_time_ns ~ cost, data=df))  
}
lm_validation(validation)
```


```{r}
validation_safe_subset = validation[which(validation$has_exp == 'blue' & validation$n_zero_args == 0),]
plot_validation(validation_safe_subset)
```

```{r}
lm_validation(validation_safe_subset)
```

```{r}
validation_problem_subset = validation[which(validation$has_exp == 'red' & validation$n_zero_args != 0),]
plot_validation(validation_problem_subset)
```

```{r}
lm_validation(validation_problem_subset)
```