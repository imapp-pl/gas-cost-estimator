---
title: "R Notebook: exploration of validation by comparing individual and total measurements; for Gas Cost Estimator"
output: html_notebook
---

```{r include=FALSE, fig.width=20}
library(sqldf)
```

## Data preparations

We're using the random arithmetic program generator via: `python3 program_generator/pg_arythmetic.py generate --count=100 --opsLimit=10 --fullCsv > ../../local/pg_arythmetic_100_10.csv`.

Then tracing it using: `docker run   --rm -v /home/user/sources/imapp/local:/srv/local  -it gas-cost-estimator/geth_all sh -c "cd src && cat /srv/local/pg_arythmetic_100_10.csv | python3 instrumentation_measurement/measurements.py measure --evm geth --mode trace --sampleSize 1 > /srv/local/trace_pg_arythmetic_100_10.csv"`.

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")

program_set_codename = "c100_ops10"

traces = read.csv(paste("../../local/trace_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(traces)
```
```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")
total_measurements = read.csv(paste("../../local/geth_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(total_measurements)
```

```{r}
# temporary workaround for obtaining the estimated cost weights
setwd("~/sources/imapp/gas-cost-estimator/src")
estimated_cost = read.csv("../../local/rough_opcode_costs_new.csv")
head(estimated_cost)
```

```{r}
ops_per_program = data.frame(table(traces$program_id, traces$op, dnn=c("program_id", "op")))

estimated_programs = sqldf("SELECT
                              program_id,
                              sum(ops_per_program.Freq * estimated_cost.mean) as avg_cost,
                              sum(ops_per_program.Freq * estimated_cost.q50) as q50_cost,
                              sum(ops_per_program.Freq * estimated_cost.min) as min_cost,
                              sum(ops_per_program.Freq * estimated_cost.current_gas) as current_gas_cost
                           FROM ops_per_program
                           INNER JOIN estimated_cost ON ops_per_program.op == estimated_cost.op
                           GROUP BY program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg(total_measurements.measure_total_time_ns) as avg_measure_total_time_ns,
                      min(total_measurements.measure_total_time_ns) as min_measure_total_time_ns,
                      avg_cost,
                      q50_cost,
                      min_cost,
                      current_gas_cost
                    FROM estimated_programs
                    INNER JOIN total_measurements ON total_measurements.program_id == estimated_programs.program_id
                    GROUP BY estimated_programs.program_id
                    ORDER BY cast(estimated_programs.program_id AS int)")

head(validation)
```

We need an additional simple (plottable) "fingerprint" of the OPCODEs included in the program, to evaluate the validation plot at a glance.

```{r}
library(nlme)
fingerprint <- function(x) {
  x[which(!(x$op %in% c('PUSH32', 'PUSH1', 'STOP'))),'op']
}
has_exp <- function(x) {
  if ('EXP' %in% x$op) 'red' else 'blue'
}
count_zero_args <- function(x) {
  sum(!is.na(x$stack_elem0) & x$stack_elem0 == 0 | !is.na(x$stack_elem1) & x$stack_elem1 == 0 | !is.na(x$stack_elem2) & x$stack_elem2 == 0)
}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'stack_elem0', 'stack_elem1', 'stack_elem2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

Basic inspection of data reveals strong decreasing trend for each sample (sequence of runs) and a lot of outliers.

```{r fig.width=20}
plot(total_measurements$measure_total_time_ns, ylim=c(0,3200))
boxplot(total_measurements$measure_total_time_ns)
boxplot(measure_total_time_ns ~ program_id, data=total_measurements, outline=FALSE)
# tapply(total_measurements$measure_total_time_ns, total_measurements$program_id, hist, probability=T)
# tapply(total_measurements$measure_total_time_ns, total_measurements$program_id, summary)
```

The relation between the estimated `cost` and the measured `avg_measure_total_time_ns` is quite evident for
"cheap" programs, but not anymore for "expensive" program in the set.
Educated guess was it is because of `EXP`, which has been measured individually with smallest argument in the Stage I, and those measurements led to the estimated `cost`.
Thereby the `EXP` programs are consistently underestimated.

```{r}
explained_variable = 'min_measure_total_time_ns'
estimate_variable = 'min_cost'

plot_validation <- function(df, yvar, xvar, label_n_zero_args) {
  if (missing(label_n_zero_args)) {
    label_n_zero_args = FALSE
  }
  scatter.smooth(df[,xvar], df[,yvar], col=df$has_exp)
  text(df[,yvar] ~ df[,xvar], data=df, labels=program_id, cex=0.6, pos=4, font=2)
  if (label_n_zero_args) {
    text(df[,yvar] ~ df[,xvar], data=df, labels=n_zero_args, cex=0.6, pos=1, font=2)
  }
}
plot_validation(validation, explained_variable, estimate_variable, label_n_zero_args=TRUE)
```

```{r}
lm_validation <- function(df, yvar, xvar) {
  summary(lm(df[, yvar] ~ df[,xvar], data=df))  
}
lm_validation(validation, explained_variable, estimate_variable)
```


```{r fig.width=10}
validation_safe_subset = validation[which(validation$has_exp == 'blue' & validation$n_zero_args == 0),]
par(mfrow=c(1,2))
plot_validation(validation_safe_subset, explained_variable, estimate_variable)
plot_validation(validation_safe_subset, explained_variable, 'current_gas_cost')
```

```{r}
model = lm_validation(validation_safe_subset, explained_variable, estimate_variable)
print(model)
print("-------------------")
lm_validation(validation_safe_subset, explained_variable, 'current_gas_cost')
```
Here, for comparison the programs that currently turn out to be problematic:

```{r}
validation_problem_subset = validation[which(validation$has_exp == 'red' | validation$n_zero_args != 0),]
plot_validation(validation_problem_subset, explained_variable, estimate_variable, label_n_zero_args=TRUE)
```

```{r}
lm_validation(validation_problem_subset, explained_variable, estimate_variable)
```

```{r}
plot(validation_safe_subset[, estimate_variable], model$residuals)
plot(validation_safe_subset[, explained_variable], model$residuals)
```

```{r}
validation_resids = validation_safe_subset
validation_resids$residuals = model$residuals
```

## Tweak the validation

Since the results are not very good so far, we try the following ideas to improve the fit:

We adjust the program set by:
1. Decreasing the size of stack arguments to 1-byte numbers, which mirrors the estimated costs (which used a fixed arg of 32)
2. Reclaiming control over them, to avoid the proliferation of 0 entries on the stack

This improved the results greatly and allowed us to keep `EXP` programs

We adjust the analysis:
1. Remove outliers from the `measure_total` results

### Clean the program set

```{r}
# save previous results, we'll reuse the names moving forward
estimated_programs_first_pass = estimated_programs
model_first_pass = model
ops_per_program_first_pass = ops_per_program
total_measurements_first_pass = total_measurements
traces_first_pass = traces
validation_first_pass = validation
validation_problem_subset_first_pass = validation_problem_subset
validation_resids_first_pass = validation_resids
validation_safe_subset_first_pass = validation_safe_subset
program_set_codename_first_pass = program_set_codename
rm('estimated_programs', 'model', 'ops_per_program',
   'total_measurements', 'traces', 'validation', 'validation_problem_subset',
   'validation_resids', 'validation_safe_subset', 'program_set_codename')

# point to the new data set
program_set_codename = "c100_ops30_clean_smallpush"
```

Redo all the calculations from the above

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")

traces = read.csv(paste("../../local/trace_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(traces)
```

```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")
total_measurements = read.csv(paste("../../local/geth_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(total_measurements)
```
### Remove outliers

```{r}
remove_outliers <- function(df, col) {
  # we don't have subsets, so the entire df is a subset
  subset = df
  outliers = boxplot(subset[, col], plot=FALSE)$out
  # NOTE here we're also not filtering by the subset
  no_outliers = df[-which(df[, col] %in% outliers), ]
  return(no_outliers)
}

total_measurements = remove_outliers(total_measurements, "measure_total_time_ns")
head(total_measurements)
```

```{r}
ops_per_program = data.frame(table(traces$program_id, traces$op, dnn=c("program_id", "op")))

estimated_programs = sqldf("SELECT
                              program_id,
                              sum(ops_per_program.Freq * estimated_cost.mean) as avg_cost,
                              sum(ops_per_program.Freq * estimated_cost.q50) as q50_cost,
                              sum(ops_per_program.Freq * estimated_cost.min) as min_cost,
                              sum(ops_per_program.Freq * estimated_cost.current_gas) as current_gas_cost
                           FROM ops_per_program
                           INNER JOIN estimated_cost ON ops_per_program.op == estimated_cost.op
                           GROUP BY program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg(total_measurements.measure_total_time_ns) as avg_measure_total_time_ns,
                      min(total_measurements.measure_total_time_ns) as min_measure_total_time_ns,
                      avg_cost,
                      q50_cost,
                      min_cost,
                      current_gas_cost
                    FROM estimated_programs
                    INNER JOIN total_measurements ON total_measurements.program_id == estimated_programs.program_id
                    GROUP BY estimated_programs.program_id
                    ORDER BY cast(estimated_programs.program_id AS int)")

head(validation)
```

```{r}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'stack_elem0', 'stack_elem1', 'stack_elem2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

Basic inspection of data reveals strong decreasing trend for each sample (sequence of runs) and a lot of outliers.

**TODO**: try increasing the sample size and see impact on the decreasing trend - will it stabilize?

```{r fig.width=20}
# we've removed outliers so picking the right ylim / OUTLINE no longer required

plot(total_measurements$measure_total_time_ns)
boxplot(total_measurements$measure_total_time_ns)
boxplot(measure_total_time_ns ~ program_id, data=total_measurements)
```

```{r}
par(mfrow=c(1,2))
plot_validation(validation, explained_variable, estimate_variable)
plot_validation(validation, explained_variable, 'current_gas_cost')
```
Try with the safer subset (no EXP) again, to check how do we compare against the current gas cost.
Current gas cost is completely off for EXP.
**NOTE**: the safe subset has different criteria vs the previous one.

```{r fig.width=10}
validation_no_exp = validation[which(validation$has_exp == 'blue'),]
par(mfrow=c(1,2))
plot_validation(validation_no_exp, explained_variable, estimate_variable)
plot_validation(validation_no_exp, explained_variable, 'current_gas_cost')
```

```{r}
model = lm_validation(validation, explained_variable, estimate_variable)
print(model)
```
**NOTE** the linear model for current gas cost doesn't make any sense with EXP included.
To compare the no-EXP subset, where the current gas cost is a decent estimate:

```{r}
lm_validation(validation_no_exp, explained_variable, estimate_variable)
lm_validation(validation_no_exp, explained_variable, 'current_gas_cost')
```

```{r}
plot(validation[, estimate_variable], model$residuals)
plot(validation[, explained_variable], model$residuals)
```

```{r}
validation_resids = validation
validation_resids$residuals = model$residuals
```