---
title: "R Notebook: exploration of validation of measurements; for Gas Cost Estimator"
output: html_notebook
---

```{r include=FALSE, fig.width=20}
library(sqldf)
library(nlme)
```

## Final validation

### Data preparations

We need an additional simple (plottable) "fingerprint" of the OPCODEs included in the program, to evaluate the validation plot at a glance.

```{r}
fingerprint <- function(x) {
  paste(x[which(!(x$op %in% c('PUSH32', 'PUSH1', 'STOP', 'POP'))),'op'], collapse=' ')
}
has_exp <- function(x) {
  if ('EXP' %in% x$op) 'red' else 'blue'
}
count_zero_args <- function(x) {
  sum(!is.na(x$stack_elem0) & x$stack_elem0 == 0 | !is.na(x$stack_elem1) & x$stack_elem1 == 0 | !is.na(x$stack_elem2) & x$stack_elem2 == 0)
}
```

```{r}
explained_variable = 'avg_measure_total_time_ns'
estimate_variable = 'q50_cost'
```

```{r}
plot_validation <- function(df, yvar, xvar, label_n_zero_args) {
  if (missing(label_n_zero_args)) {
    label_n_zero_args = FALSE
  }
  scatter.smooth(df[,xvar], df[,yvar], col=df$has_exp)
  text(df[,yvar] ~ df[,xvar], data=df, labels=program_id, cex=0.6, pos=4, font=2)
  if (label_n_zero_args) {
    text(df[,yvar] ~ df[,xvar], data=df, labels=n_zero_args, cex=0.6, pos=1, font=2)
  }
}
```

```{r}
lm_validation <- function(df, yvar, xvar, selection_col, selection_val) {
  if (! missing(selection_col) & ! missing(selection_val)) {
    subset = df[which(df[, selection_col] == selection_val), ]
  } else {
    subset = df
  }
  lm(subset[, yvar] ~ subset[,xvar], data=subset)
}
```

### Clean the program set

```{r}
# point to the new data set
program_set_codename = "c100_ops30_clean_smallpush"
```

Redo all the calculations from the above

```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")

estimated_cost = read.csv("../../local/estimated_cost.csv")
current_gas_cost = read.csv("../../local/current_gas_cost.csv")
# from measure_marginal.Rmd
marginal_estimated_cost = read.csv("../../local/marginal_estimated_cost.csv")

head(estimated_cost)
head(marginal_estimated_cost)
head(current_gas_cost)
```

We're using the random arithmetic program generator via:

```
python3 program_generator/pg_arythmetic.py generate --count=100 --opsLimit=30 --cleanStack --push=1 --fullCsv > ../../local/pg_arythmetic_100_10.csv`.
```

Tracing it using:

```
docker run \
  --rm -v /home/user/sources/imapp/local:/srv/local \
  -it gas-cost-estimator/geth_all \
  sh -c "cd src && cat <program set filename> | python3 instrumentation_measurement/measurements.py measure --evm geth --mode trace --sampleSize 1"`.
```

Measure the program set in `measure_total` mode using:

```
docker run \
  --rm   --privileged --security-opt seccomp:unconfined \
  -v /home/ubuntu/pdobacz/local:/srv/local \
  -it gas-cost-estimator/geth_total \
  sh -c "cd src && cat <program set filename> | python3 instrumentation_measurement/measurements.py measure --evm geth --mode total --sampleSize=50 --nSamples=1"
```

We've extended the program length to 30 OPCODEs to accommodate more `PUSH`es and `POP`s.

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")

traces = read.csv(paste("../../local/trace_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(traces)
total_measurements_geth = read.csv(paste("../../local/geth_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(total_measurements_geth)
total_measurements_evmone = read.csv(paste("../../local/evmone_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(total_measurements_evmone)
```

### Remove outliers

```{r}
remove_outliers <- function(df, col) {
  # we don't have subsets, so the entire df is a subset
  subset = df
  outliers = boxplot(subset[, col], plot=FALSE)$out
  # NOTE here we're also not filtering by the subset
  no_outliers = df[-which(df[, col] %in% outliers), ]
  return(no_outliers)
}

total_measurements_geth = remove_outliers(total_measurements_geth, "measure_total_time_ns")
head(total_measurements_geth)
total_measurements_evmone = remove_outliers(total_measurements_evmone, "measure_total_time_ns")
head(total_measurements_evmone)
```

```{r}
total_measurements_geth$env = 'geth'
total_measurements_evmone$env = 'evmone'
total_measurements = rbind(total_measurements_geth, total_measurements_evmone)
head(total_measurements)
```
### Validation using improved data/method

```{r}
ops_per_program = data.frame(table(traces$program_id, traces$op, dnn=c("program_id", "op")))

estimated_programs = sqldf("SELECT
                              program_id,
                              sum(ops_per_program.Freq * estimated_cost.mean) as avg_cost,
                              sum(ops_per_program.Freq * estimated_cost.q50) as q50_cost,
                              sum(ops_per_program.Freq * estimated_cost.min) as min_cost,
                              sum(ops_per_program.Freq * current_gas_cost.current_gas) as current_gas_cost,
                              sum(ops_per_program.Freq * marginal_estimated_cost.estimate_marginal_ns) as marginal_cost,
                              estimated_cost.env
                           FROM ops_per_program
                           INNER JOIN estimated_cost ON
                                 ops_per_program.op == estimated_cost.op
                           INNER JOIN current_gas_cost ON
                                 ops_per_program.op == current_gas_cost.op
                           LEFT JOIN marginal_estimated_cost ON
                                ops_per_program.op == marginal_estimated_cost.op 
                            AND marginal_estimated_cost.env == estimated_cost.env
                           GROUP BY program_id, estimated_cost.env")

aggregate_measurements = sqldf("SELECT
                                  program_id,
                                  env,
                                  avg(measure_total_time_ns) as avg_measure_total_time_ns,
                                  min(measure_total_time_ns) as min_measure_total_time_ns
                                FROM total_measurements
                                GROUP BY env, program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg_measure_total_time_ns,
                      min_measure_total_time_ns,
                      avg_cost,
                      q50_cost,
                      min_cost,
                      current_gas_cost,
                      marginal_cost,
                      estimated_programs.env
                    FROM estimated_programs
                    INNER JOIN aggregate_measurements ON 
                          aggregate_measurements.program_id == estimated_programs.program_id
                      AND aggregate_measurements.env == estimated_programs.env
                    ORDER BY estimated_programs.env, cast(estimated_programs.program_id AS int)")

head(validation)
```

```{r}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'stack_elem0', 'stack_elem1', 'stack_elem2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

Basic inspection of data reveals strong decreasing trend for each sample (sequence of runs).
The outliers have been removed, so we show what we consider outliers now in the clean sample.
They seem under control.

```{r fig.width=15}
# we've removed outliers so picking the right ylim / OUTLINE no longer required

overview_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  plot(subset$measure_total_time_ns, main=selection_val)
  boxplot(measure_total_time_ns ~ program_id, data=subset, main=selection_val)
  boxplot(measure_total_time_ns ~ run_id, data=subset, las=2, main=selection_val)
}
overview_plots(total_measurements, 'env', 'geth')
overview_plots(total_measurements, 'env', 'evmone')
```

Here is a detailed inspection of the "warm-up" profiles for first 40 programs.
Each program seems to warm-up the same.

```{r fig.width=15, fig.height=30}
warmup_profile_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  n_programs = 40
  n_plot_cols = 4
  n_plot_rows = n_programs %/% n_plot_cols + 1
  par(mfrow=c(n_plot_rows, n_plot_cols))
  program_id=0
  for (program_id in 0:n_programs)
  {
    program_measurements = subset[which(subset$program_id==program_id), ]
    plot(program_measurements$measure_total_time_ns, main=selection_val)
  }
}
warmup_profile_plots(total_measurements, 'env', 'geth')
warmup_profile_plots(total_measurements, 'env', 'evmone')
print("Hide big plots")
```
### Check estimation using trivial variables

Based on the plots alone, it does not appear like our estimation only relies on trivial variables.

```{r}
program_length <- function(x) {
  max(x$instruction_id)
}
count_op <- function(x, op) {
  sum(x$op == op)
}
# the "trivial" variables to check if we are not doing trivial estimation
validation$program_length = gapply(traces, c('program_id', 'instruction_id'), FUN=function(x) program_length(x), groups=traces$program_id)
validation$n_push = gapply(traces, c('program_id', 'op'), FUN=function(x) count_op(x, 'PUSH1'), groups=traces$program_id)
validation$n_pop = gapply(traces, c('program_id', 'op'), FUN=function(x) count_op(x, 'POP'), groups=traces$program_id)
```

```{r}
trivial_variable_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  par(mfrow=c(1,3))
  plot_validation(subset, explained_variable, 'program_length')
  title(main=selection_val)
  plot_validation(subset, explained_variable, 'n_push')
  title(main=selection_val)
  plot_validation(subset, explained_variable, 'n_pop')
  title(main=selection_val)
}
trivial_variable_plots(validation, 'env', 'geth')
trivial_variable_plots(validation, 'env', 'evmone')
```

```{r}
compare_current_gas_cost_plots <- function(df, selection_col, selection_val, estimate_var) {
  subset = df[which(df[, selection_col] == selection_val), ]
  par(mfrow=c(1,2))
  plot_validation(subset, explained_variable, estimate_var)
  title(main=paste(selection_val, ' estimate'))
  plot_validation(subset, explained_variable, 'current_gas_cost')
  title(main=paste(selection_val, ' current gas'))
}
compare_current_gas_cost_plots(validation, 'env', 'geth', estimate_variable)
compare_current_gas_cost_plots(validation, 'env', 'evmone', estimate_variable)
```

Try with the safer subset (no EXP) again, to check how do we compare against the current gas cost.
Current gas cost is completely off for EXP.
**NOTE**: the safe subset has different criteria vs the previous one.

```{r fig.width=10}
validation_no_exp = validation[which(validation$has_exp == 'blue'),]
compare_current_gas_cost_plots(validation_no_exp, 'env', 'geth', estimate_variable)
compare_current_gas_cost_plots(validation_no_exp, 'env', 'evmone', estimate_variable)
```

### Final validation model

```{r}
model_geth = lm_validation(validation, explained_variable, estimate_variable, selection_col='env', selection_val='geth')
model_evmone = lm_validation(validation, explained_variable, estimate_variable, selection_col='env', selection_val='evmone')
print('geth')
summary(model_geth)
print('evmone')
summary(model_evmone)
```

#### Model coefficients: discussion

See later section for the model diagnostics; we're assuming here that the linear model makes sense.

Under such assumption, the model tells us that for every nanoseconds spent when calculating the OPCODE time during the individual measurements (our estimation), a larger program spends (see **`Coefficients`** section of the above summary):

1. `0.90351 ns` with `geth`
2. `0.60581 ns` with `evmone`

It is not surprising that the larger program calculates the OPCODE more efficiently.
It might be that the "dense" invocation of timer calls is altering the processing pipeline.

This coefficient might capture the "penalty" we're incurring when measuring the OPCODE execution individually.
It seems to be on an acceptable level, however, we must pay attention to whether this "penalty" isn't OPCODE-dependent **TODO**.
If it were, it could mean that the estimated costs are not "fair" across OPCODEs.

Another aspect to investigate is the **`Intercept`**, which could be seen as the "estimated cost of an empty program".

For `evmone` it is statistically insignificant, so can assume it's zero.
For `geth` it is larger, but on a level which can be attributed to warmup effects which `geth` exhibits.

##### Earlier "large" `Intercept` problem

Before we updated `geth` to `v1.10.13`, the `Intercept` was much larger.
Here's the original description of the problem noticed:

For `geth` the `Intercept` was large, `930 ns`.
It cannot be explained by warm-up, which seems to amount to maximum of around `400 ns` for a couple of first measurement runs.
We've ran the same program set using the `measure_all` method (we measure all individual OPCODE time).

Quick ideas, what could contribute to this intercept:
1. some lazy initialization of something inside `geth` / `golang` (but this seems like much smaller impact ~`70 ns`)
2. the impact of non-linearity of the data offsetting the model "upwards" (again, this shouldn't be that much)
3. warm-up (as mentioned, not big enough)
4. some silly error

### Validation model using marginal cost

Using the results from `measure_marginal.Rmd`.
We have a statistically significant intercept, which disappears if we exclude `EXP` (see next chunk).

Also, for every nanosecond spent calculating the marginal OPCODE, we spend `3.7` and `1.7` nanoseconds calculating the random program OPCODE.
We would be expecting it be closer to `1`, excluding `EXP` doesn't help much in this aspect.
**TODO**: optionally, try to make it closer to `1`.

```{r}

compare_current_gas_cost_plots(validation, 'env', 'geth', 'marginal_cost')
compare_current_gas_cost_plots(validation, 'env', 'evmone', 'marginal_cost')
compare_current_gas_cost_plots(validation_no_exp, 'env', 'geth', 'marginal_cost')
compare_current_gas_cost_plots(validation_no_exp, 'env', 'evmone', 'marginal_cost')

model_geth_marginal = lm_validation(validation, explained_variable, 'marginal_cost', selection_col='env', selection_val='geth')
model_evmone_marginal = lm_validation(validation, explained_variable, 'marginal_cost', selection_col='env', selection_val='evmone')
print('geth')
summary(model_geth_marginal)
print('evmone')
summary(model_evmone_marginal)
```

Comparison models excluding `EXP` - no strange intercept for `geth` anymore.
We probably must parametrize `EXP` arguments carefully.

```{r}
summary(lm_validation(validation_no_exp, explained_variable, 'marginal_cost', selection_col='env', selection_val='geth'))
summary(lm_validation(validation_no_exp, explained_variable, 'marginal_cost', selection_col='env', selection_val='evmone'))
```

Compare OPCODE estimates from both methods directly

```{r fig.width=14}
estimate_comparison = sqldf("SELECT estimated_cost.*, marginal_estimated_cost.estimate_marginal_ns
                             FROM marginal_estimated_cost
                             LEFT JOIN estimated_cost ON estimated_cost.op = marginal_estimated_cost.op
                                  AND estimated_cost.env = marginal_estimated_cost.env")
head(estimate_comparison)
scatter.smooth(estimate_comparison$estimate_marginal_ns ~ estimate_comparison$min, log="xy")
text(estimate_comparison$estimate_marginal_ns ~ estimate_comparison$min, labels=paste(estimate_comparison$op, estimate_comparison$env), cex=0.6, pos=1, font=2)
abline(0, 1, col="red")
```

## Comparison of estimated values with `measure_all` measurements

We measure each and every OPCODE of the random validation programs, to see, if we can spot some differences in how the OPCODEs behave.

```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")
measure_all_geth = read.csv(paste("../../local/geth_pg_arythmetic_", program_set_codename, "_measure_all", ".csv", sep=""))
head(measure_all_geth)
```
```{r}
measure_all_geth_details = sqldf("SELECT
                                  traces.instruction_id,
                                  traces.program_id,
                                  avg(measure_all_time_ns - 22) as avg_opcode_time_ns,
                                  min(measure_all_time_ns - 22) as min_opcode_time_ns,
                                  estimated_cost.*
                                FROM measure_all_geth
                                INNER JOIN traces ON
                                     measure_all_geth.instruction_id = traces.instruction_id
                                 AND measure_all_geth.program_id = traces.program_id
                                INNER JOIN estimated_cost ON
                                     traces.op = estimated_cost.op
                                WHERE estimated_cost.env = 'geth'
                                GROUP BY measure_all_geth.program_id, measure_all_geth.instruction_id
                                ")
head(measure_all_geth_details)
```

```{r}
# the -22 is coming from the other notebook, this is the rough estimate of the
# timer overhead for the individual OPCODE measurements
measure_all_geth_sums = sqldf("SELECT
                                 program_id,
                                 sample_id,
                                 run_id,
                                 sum(measure_all_time_ns - 22) as sum_program_time_ns
                                FROM measure_all_geth
                                GROUP BY program_id, sample_id, run_id
                              ")

```

```{r}
measure_all_geth_sum_stats = sqldf("SELECT
                                      measure_all_geth_sums.program_id,
                                      avg(sum_program_time_ns) as avg_sum_program_time_ns,
                                      min(sum_program_time_ns) as min_sum_program_time_ns,
                                      validation.avg_measure_total_time_ns,
                                      validation.min_measure_total_time_ns,
                                      (avg(sum_program_time_ns) - validation.avg_measure_total_time_ns) as diff_avg,
                                      (min(sum_program_time_ns) - validation.min_measure_total_time_ns) as diff_min
                                    FROM measure_all_geth_sums
                                    INNER JOIN validation ON
                                          measure_all_geth_sums.program_id = validation.program_id
                                    WHERE validation.env = 'geth'
                                    GROUP BY measure_all_geth_sums.program_id
                                  ")
head(measure_all_geth_sum_stats)
```
```{r}
measure_all_geth_sum_stats$prediction = predict(model_geth, validation[which(validation$env == 'geth'), ])
```

## Compare model with current gas schedule

**NOTE** the linear model for current gas cost doesn't make any sense with EXP included.
To compare the no-EXP subset, where the current gas cost is a decent estimate:

```{r}
print('geth, our estimate')
summary(lm_validation(validation_no_exp, explained_variable, estimate_variable, selection_col='env', selection_val='geth'))
print('geth, marginal_cost')
summary(lm_validation(validation_no_exp, explained_variable, 'marginal_cost', selection_col='env', selection_val='geth'))
print('geth, gas schedule')
summary(lm_validation(validation_no_exp, explained_variable, 'current_gas_cost', selection_col='env', selection_val='geth'))
print('evmone, our estimate')
summary(lm_validation(validation_no_exp, explained_variable, estimate_variable, selection_col='env', selection_val='evmone'))
print('evmone, marginal_cost')
summary(lm_validation(validation_no_exp, explained_variable, 'marginal_cost', selection_col='env', selection_val='evmone'))
print('evmone, gas schedule')
summary(lm_validation(validation_no_exp, explained_variable, 'current_gas_cost', selection_col='env', selection_val='evmone'))
```
## Model diagnostics

The linear model metrics are reasonable. Things to watch out for:
1. ~Slight non-linearity for `geth` (costlier programs overestimated)~. EDIT: not anymore
2. ~Non-normality for `evmone`~. EDIT: not anymore

The diagnosticts look decent.

```{r fig.width=15}
par(mfrow=c(4,4))
plot(model_geth)
plot(model_evmone)
plot(model_geth_marginal)
plot(model_evmone_marginal)
```