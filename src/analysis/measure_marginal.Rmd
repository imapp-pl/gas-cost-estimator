---
title: "R Notebook: estimation of gas costs using the measure marginal method; for Gas Cost Estimator"
output: html_document
---

In this script we conduct the estimation for the `measure_marginal` approach.

```{r, setup, include=FALSE}
# NOTE: change this to your respective path
knitr::opts_knit$set(root.dir = '~/sources/imapp/gas-cost-estimator/src')
source('common.R')
```

## Data preparations

`programs.csv` as generated by `pg_marginal.py` with `--fullCSV` flag on.

```{r}
program_set_codename = "pg_marginal_full5_c50_step1_shuffle"
programs = read.csv(paste("../../local/", program_set_codename, ".csv", sep=""))
```

`result_*.csv` as generated by `src/measurements.py`. 
Invocation similar to: `make measure EVM=geth VOLUME_DIR=/home/user/sources/imapp/local PROGRAMS=pg_marginal_full5_c50_step1_shuffle SAMPLESIZE=50 NSAMPLES=4`. 

```{r fig.width=20}
suffix = ""
# suffix = "_lenovo"
measurement_codename = paste0("50_4", suffix)

result_geth = load_data_set("geth", program_set_codename, measurement_codename)
result_evmone = load_data_set("evmone", program_set_codename, measurement_codename)
# TODO geth short-circuits zero length programs, resulting in zero timing somehow. Drop these more elegantly, not based on measure_total_time_ns
result_geth = result_geth[which(result_geth$measure_total_time_ns != 0), ]
all_envs = c("geth", "evmone")
results = rbind(result_evmone, result_geth)
```

Combine these data frames into our main `measurements`, which will combine the measurements and the traits of the measured programs.

```{r}
measurements = sqldf("SELECT opcode, op_count, sample_id, run_id, measure_total_time_ns, env, results.program_id
                     FROM results
                     INNER JOIN
                       programs ON(results.program_id = programs.program_id)")
measurements$opcode = factor(measurements$opcode, levels=unique(programs$opcode))
head(measurements)
```

### Remove outliers

Removing or not of the outliers has a large impact on the final results.
It seems that for `EXP` and `evmone` there are many outliers in the mid-low `op_count` samples.
This effectively "leverages" the estimated slope downwards, pretending `EXP`'s `evmone` to be cheaper than it really seems to be.

Switch `removed_outliers` to `FALSE` to see the comparison.

```{r fig.width=15}
removed_outliers = TRUE

if (removed_outliers) {
  measurements = remove_compare_outliers(measurements, 'measure_total_time_ns', all_envs)
}
```

Use the plots above as the first line of sanity check - different OPCODEs should have their programs execution last a varying amount of time, there should be no surprising entries in terms of variance and outliers.

### Functions

```{r}
# For a subset of the `measurements` data frame, fits a bimodal distribution model and corrects the
# data by bringing the "top-mode" cluster down to the "bottom-mode" cluster.
correct_bimodal <- function(df) {
  mix_model = normalmixEM(df$measure_total_time_ns)
  print(summary(mix_model))
  plot(mix_model,which=2)
  mode_distance = abs(mix_model$mu[2] - mix_model$mu[1])
  mode_midpoint = (mix_model$mu[2] + mix_model$mu[1]) / 2
  over_threshold = which(df$measure_total_time_ns > mode_midpoint)
  df[over_threshold, "measure_total_time_ns"] = df[over_threshold, "measure_total_time_ns"] - mode_distance
    
  return(df)
}

# Performs the `measure_marginal` estimation procedure for a given slice of the data.
# Prints the diagnostics and plots the models.
compute_all <- function(opcode, env, plots, bimodal_opcodes, use_median) {
  if (missing(bimodal_opcodes)) {
    bimodal_opcodes = c()
  }
  if (missing(plots)) {
    plots = "scatter"
  }
  if (missing(use_median)) {
    use_median = FALSE
  }
  print(c(opcode, env))
  
  df = measurements[which(measurements$opcode==opcode & measurements$env==env),]
  
  if (opcode %in% bimodal_opcodes) {
    par(mfrow=c(1,2))
    boxplot(measure_total_time_ns ~ op_count, data=df, las=2, outline=removed_outliers)
    title(main=paste(env, opcode))
    # correct_bimodal plots the second plot inside
    df = correct_bimodal(df)
  }
  
  if (use_median) {
    f = median
  } else {
    f = mean
  }
  df_mean = aggregate(measure_total_time_ns ~ op_count * env, df, f)
  
  model_mean = lm(measure_total_time_ns ~ op_count, data=df_mean)
  print(summary(model_mean))
  slope = model_mean$coefficients[['op_count']]
  stderr = summary(model_mean)$coefficients['op_count','Std. Error']
  
  if (plots == "scatter" | plots == "all") {
    par(mfrow=c(1,1))
    boxplot(measure_total_time_ns ~ op_count, data=df, las=2, outline=removed_outliers)
    rounded_slope = round(slope, 3)
    rounded_p = round(summary(model_mean)$coefficients['op_count','Pr(>|t|)'], 3)
    rounded_stderr = round(stderr, 3)
    title(main=paste(env, opcode, rounded_slope, "p_value:", rounded_p, "StdErr:", rounded_stderr))
    abline(model_mean, col="red")
  }
  if (plots == "diagnostics" | plots == "all") {
    par(mfrow=c(2,2))
    plot(model_mean)
  }
  list("slope" = slope, "stderr" = stderr)
}

extract_opcodes <- function() {
  unique(programs$opcode)
}
```

## `measure_marginal` procedure for all OPCODEs

```{r}
# We use 50 growing programs, instead of the initial 200:
# 1. it appears to stabilize fast enough, after we have begun randomizing the order of programs to avoid impact of periodic slowness
# 2. `geth` is very slow to calculate and blows the CPU allowance on our AWS machine, if we do 200
# 3. for `openethereum` we should be using use programs with 0-200 increasing `op_count`, otherwise the model doesn't capture the trend. Possibly, this will be resolved by dropping `openethereum`
all_opcodes = extract_opcodes()

# initialize the data frame to hold the results
estimates = data.frame(matrix(ncol = 4, nrow = 0))
colnames(estimates) <- c('op', 'estimate_marginal_ns', 'estimate_marginal_ns_stderr', 'env')
```

```{r}
estimates = estimates[which(estimates$env != 'geth'), ]
for (opcode in all_opcodes) {
  estimate = compute_all(opcode, "geth", use_median=TRUE, plots='all')
  estimates[nrow(estimates) + 1, ] = c(opcode, estimate$slope, estimate$stderr, 'geth')
}
```
Use the above plots (if needed, refer also to the printed summaries of the `lm` results), to validate that the `p-values` are zero everywhere, and that the observation boxplots are arranged to follow the plotted trend line.
Spot any abnormalities here (like the bimodality for the `evmone` in the next batch of results below).

```{r}
evmone_bimodals = all_opcodes[which(grepl("PUSH", all_opcodes) & all_opcodes != "PUSH1" | all_opcodes == "JUMP")]
estimates = estimates[which(estimates$env != 'evmone'), ]
for (opcode in all_opcodes) {
  estimate = compute_all(opcode, "evmone", bimodal_opcodes=evmone_bimodals, use_median=TRUE, plots='all')
  estimates[nrow(estimates) + 1, ] = c(opcode, estimate$slope, estimate$stderr, 'evmone')
}
```

### Export the results

```{r}
write.csv(estimates, paste0("../../local/marginal_estimated_cost", suffix, ".csv"), quote=FALSE, row.names=FALSE)
```

