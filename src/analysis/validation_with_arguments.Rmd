---
title: "R Notebook: exploration of validation of measurements - with varying arguments; for Gas Cost Estimator"
output: html_notebook
---

```{r include=FALSE, fig.width=20}
library(sqldf)
library(nlme)

# prevent scientific notation
options(scipen = 100)
```

### Data preparations

```{r}
explained_variable = 'avg_measure_total_time_ns'
all_envs = c('geth', 'evmone', 'openethereum')
```

```{r}
plot_validation <- function(df, yvar, xvar, label_n_zero_args, set_asp) {
  if (missing(label_n_zero_args)) {
    label_n_zero_args = FALSE
  }
  if (missing(set_asp) || set_asp == FALSE) {
    asp=NULL
  } else {
    asp = 1
  }
  scatter.smooth(df[,xvar], df[,yvar], col=df$has_exp, asp=asp)
  text(df[,yvar] ~ df[,xvar], data=df, labels=program_id, cex=0.6, pos=4, font=2)
  if (label_n_zero_args) {
    text(df[,yvar] ~ df[,xvar], data=df, labels=n_zero_args, cex=0.6, pos=1, font=2)
  }
}
```

```{r}
lm_validation <- function(df, yvar, xvar, selection_col, selection_val) {
  if (! missing(selection_col) & ! missing(selection_val)) {
    subset = df[which(df[, selection_col] == selection_val), ]
  } else {
    subset = df
  }
  lm(subset[, yvar] ~ subset[,xvar], data=subset)
}
```

```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")

estimated_cost = read.csv("../../local/argument_estimated_cost.csv")
# from measure_marginal.Rmd
# TODO we need this to fill in missing PUSHes and POP, to be rectified later
marginal = read.csv("../../local/marginal_estimated_cost.csv")

# TODO remove when rectified
for (opcode in c('PUSH1', 'POP')) {
  for (env in all_envs) {
    # working around the ridiculous type coercion of `c()`
    estimated_cost[nrow(estimated_cost) + 1, 1:2] = c(opcode, env)
    estimated_cost[nrow(estimated_cost), 3:4] = c(FALSE, FALSE)
    estimated_cost[nrow(estimated_cost), 5:8] = c(marginal[which(marginal$op==opcode & marginal$env==env), 'estimate_marginal_ns'], NA, NA, NA)
  }
}
```

#### Mix marginal and argument measurements?

We originally used only the results coming from the "arguments" measurements. This means that the programs used featured running the OPCODEs with varying arguments (spanning the entire space of uint256).
For the constant cost of an OPCODE, we'd take the `op_count` variable estimation.
This turned out to give very coarse estimate of the constant cost (clustered estimations for evmone, grossly overestimated constant cost of MOD and friends).

We next tried to "mix" the two results. Take the constant cost estimated via "measure marginal" (simple, constant arguments, varying length of a series of OPCODEs) and take the estimation of the argument impact from the "measure argument" approach.

The drawbacks of the mixed approach are:
  - might be OPCODE-unfair
  - give slightly less healthy results (with the only-"arguments", the slope coefficients are closer to 1)
  - is more complicated
  - after the fixes to the "arguments" measurement routine, as well as switching to the `c100_ops300_clean` validation set (which doesn't limit args to `PUSH1`-size), the only-"arguments" method doesn't give bad results anymore
  
Therefore, in the end, we stick with only-"arguments" measurements, and leave "marginal" measurements for cross validation only.

```{r}
order1 = order(estimated_cost$opcode, estimated_cost$env)
order2 = order(marginal$op, marginal$env)

# commented out - we **don't** mix
# estimated_cost[order1, 'estimate_marginal_ns'] = marginal[order2, 'estimate_marginal_ns']

head(estimated_cost)
head(marginal)
```

See `individual_vs_total_validation.Rmd` for invocations to generate/trace/measure programs.

We've extended the program length to 30 OPCODEs to accommodate more `PUSH`es and `POP`s.

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")

# we have tried `c900_ops300_clean` too, but it is a bit overkill. No surprises there.
# program_set_codename = "c100_ops300_clean"
# measurement_codename = "200_8"

# we randomize the number of instructions in the random programs to capture program length impact on OPCODE performance
program_set_codename = "c100_randOps500_clean"
measurement_codename = "200_8"

# this dataset (tiny programs) has a radically highler slope coefficient and lower Intercept
# this prompt us to vary the opsLimit for the validation program generation
# program_set_codename = "c100_ops10_clean"
# measurement_codename = "200_8"

# this is the data set without varying argument size. 
# program_set_codename = "c100_ops30_clean_smallpush"
# measurement_codename = "50_1"

traces = read.csv(paste("../../local/trace_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(traces)
total_measurements_geth = read.csv(paste("../../local/geth_pg_arythmetic_", program_set_codename, "_", measurement_codename, ".csv", sep=""))
head(total_measurements_geth)
total_measurements_evmone = read.csv(paste("../../local/evmone_pg_arythmetic_", program_set_codename, "_", measurement_codename, ".csv", sep=""))
head(total_measurements_evmone)
# TODO: at this point, for openethereum we are getting much better (acceptable) results for the sample size 16, nsamples 400, see Discussion section below
total_measurements_openethereum = read.csv(paste("../../local/openethereum_pg_arythmetic_", program_set_codename, "_", "16_400", ".csv", sep=""))
head(total_measurements_openethereum)

total_measurements_geth$env = 'geth'
total_measurements_evmone$env = 'evmone'
total_measurements_openethereum$env = 'openethereum'
measurements = rbind(total_measurements_geth, total_measurements_evmone, total_measurements_openethereum)
head(measurements)
```

### Remove outliers

```{r}
remove_outliers <- function(df, col) {
  boxplot_result = boxplot(df[, col] ~ df[, 'program_id'] + df[, 'env'], plot=FALSE)
  outliers = boxplot_result$out
  names = boxplot_result$names[boxplot_result$group]
  all_row_identifiers = paste(df[, col], df[, 'program_id'], df[, 'env'], sep='.')
  outlier_row_identifiers = paste(outliers, names, sep='.')
  no_outliers = df[-which(all_row_identifiers %in% outlier_row_identifiers), ]
  return(no_outliers)
}

removed_outliers = TRUE

if (removed_outliers) {
  # before
  par(mfrow=c(1,3))
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'geth'), ], las=2, outline=TRUE, log='y')
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'evmone'), ], las=2, outline=TRUE, log='y')
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'openethereum'), ], las=2, outline=TRUE, log='y')

  measurements = remove_outliers(measurements, 'measure_total_time_ns')
  
  # after
  par(mfrow=c(1,3))
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'geth'), ], las=2, outline=TRUE, log='y')
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'evmone'), ], las=2, outline=TRUE, log='y')
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'openethereum'), ], las=2, outline=TRUE, log='y')
}
```

```{r}
estimated_cost$arg0_ns[which(is.na(estimated_cost$arg0_ns))] = 0
estimated_cost$arg1_ns[which(is.na(estimated_cost$arg1_ns))] = 0
estimated_cost$arg2_ns[which(is.na(estimated_cost$arg2_ns))] = 0

estimated_traces = sqldf("SELECT
                            program_id, sample_id, instruction_id, op,
                            arg_0 as arg0, arg_1 as arg1, arg_2 as arg2,
                            env,
                            arg0_ns, arg1_ns, arg2_ns, estimate_marginal_ns
                          FROM traces
                          INNER JOIN estimated_cost ON
                            traces.op == estimated_cost.opcode")

# TODO: very crude, how can this be made more robust?

estimated_traces$arg0[which(is.na(estimated_traces$arg0))] = 0
estimated_traces$arg1[which(is.na(estimated_traces$arg1))] = 0
estimated_traces$arg2[which(is.na(estimated_traces$arg2))] = 0

estimated_traces$cost_ns = estimated_traces$estimate_marginal_ns + 
                           estimated_traces$arg0_ns * log(estimated_traces$arg0 + 1, 256) +
                           estimated_traces$arg1_ns * log(estimated_traces$arg1 + 1, 256) +
                           estimated_traces$arg2_ns * log(estimated_traces$arg2 + 1, 256)
```

```{r}
estimated_programs = sqldf("SELECT
                              program_id,
                              sum(cost_ns) as cost_ns,
                              env
                           FROM estimated_traces
                           GROUP BY program_id, env")

aggregate_measurements = sqldf("SELECT
                                  program_id,
                                  env,
                                  avg(measure_total_time_ns) as avg_measure_total_time_ns,
                                  min(measure_total_time_ns) as min_measure_total_time_ns
                                FROM measurements
                                GROUP BY env, program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg_measure_total_time_ns,
                      min_measure_total_time_ns,
                      cost_ns,
                      estimated_programs.env
                    FROM estimated_programs
                    INNER JOIN aggregate_measurements ON 
                          aggregate_measurements.program_id == estimated_programs.program_id
                      AND aggregate_measurements.env == estimated_programs.env
                    ORDER BY estimated_programs.env, cast(estimated_programs.program_id AS int)")

head(validation)
```

We need an additional simple (plottable) "fingerprint" of the OPCODEs included in the program, to evaluate the validation plot at a glance.

```{r}
fingerprint <- function(x) {
  paste(x[which(!(x$op %in% c('PUSH32', 'PUSH1', 'STOP', 'POP'))),'op'], collapse=' ')
}
has_exp <- function(x) {
  if ('EXP' %in% x$op) 'red' else 'blue'
}
count_zero_args <- function(x) {
  sum(!is.na(x$arg_0) & x$arg_0 == 0 | !is.na(x$arg_1) & x$arg_1 == 0 | !is.na(x$arg_2) & x$arg_2 == 0)
}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'arg_0', 'arg_1', 'arg_2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

```{r fig.width=15}
# we've removed outliers so picking the right ylim / OUTLINE no longer required
par(mfrow=c(2,3))
overview_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  boxplot(measure_total_time_ns ~ program_id, data=subset, main=selection_val)
  boxplot(measure_total_time_ns ~ run_id, data=subset, las=2, main=selection_val)
}
overview_plots(measurements, 'env', 'geth')
overview_plots(measurements, 'env', 'evmone')
overview_plots(measurements, 'env', 'openethereum')
```

### Check estimation using trivial variables

Based on the plots alone, it does not appear like our estimation only relies on trivial variables.

```{r}
program_length <- function(x) {
  max(x$instruction_id)
}
count_op <- function(x, op) {
  sum(x$op == op)
}
# the "trivial" variables to check if we are not doing trivial estimation
validation$program_length = gapply(traces, c('program_id', 'instruction_id'), FUN=function(x) program_length(x), groups=traces$program_id)
validation$n_push = gapply(traces, c('program_id', 'op'), FUN=function(x) count_op(x, 'PUSH1'), groups=traces$program_id)
validation$n_pop = gapply(traces, c('program_id', 'op'), FUN=function(x) count_op(x, 'POP'), groups=traces$program_id)
```

We divide the validation data into classes with respect to the length of the program:

```{r}
validation_long = validation[which(validation$program_length > 400), ]
validation_short = validation[which(validation$program_length <= 100), ]
```

```{r fig.width=15}
par(mfrow=c(3,3))
trivial_variable_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  plot_validation(subset, explained_variable, 'program_length')
  title(main=paste(selection_val, 'program_length'))
  plot_validation(subset, explained_variable, 'n_push')
  title(main=paste(selection_val, 'n_push'))
  plot_validation(subset, explained_variable, 'n_pop')
  title(main=paste(selection_val, 'n_pop'))
}
trivial_variable_plots(validation, 'env', 'geth')
trivial_variable_plots(validation, 'env', 'evmone')
trivial_variable_plots(validation, 'env', 'openethereum')
```
Trivial variable plots are interesting, after we have introduced varying program length. `openethereum` in particular has a good correlation there, which is caused by the large, constant cost of running an instruction (~250ns, regardless of OPCODE).

Nevertheless, neither of the models is comparable with the model including the estimated `cost_ns`, see below.

### Final validation model

We explore the models in two classes of program lengths. We keep the model for the long programs.

```{r}
compare_plots <- function(df, selection_col, selection_val, estimate_var) {
  subset = df[which(df[, selection_col] == selection_val), ]
  plot_validation(subset, explained_variable, estimate_var, set_asp=TRUE)
  title(main=paste(selection_val, ' estimate'))
}

model_geth = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
model_evmone = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')
model_openethereum = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='openethereum')
print('geth all')
summary(model_geth)
print('evmone all')
summary(model_evmone)
print('openethereum all')
summary(model_openethereum)
model_geth = lm_validation(validation_short, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
model_evmone = lm_validation(validation_short, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')
model_openethereum = lm_validation(validation_short, explained_variable, 'cost_ns', selection_col='env', selection_val='openethereum')
print('geth short')
summary(model_geth)
print('evmone short')
summary(model_evmone)
print('openethereum short')
summary(model_openethereum)
model_geth = lm_validation(validation_long, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
model_evmone = lm_validation(validation_long, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')
model_openethereum = lm_validation(validation_long, explained_variable, 'cost_ns', selection_col='env', selection_val='openethereum')
print('geth long')
summary(model_geth)
print('evmone long')
summary(model_evmone)
print('openethereum long')
summary(model_openethereum)
```

```{r fig.width=15}
par(mfrow=c(3,3))
compare_plots(validation, 'env', 'geth', 'cost_ns')
compare_plots(validation, 'env', 'evmone', 'cost_ns')
compare_plots(validation, 'env', 'openethereum', 'cost_ns')
compare_plots(validation_short, 'env', 'geth', 'cost_ns')
compare_plots(validation_short, 'env', 'evmone', 'cost_ns')
compare_plots(validation_short, 'env', 'openethereum', 'cost_ns')
compare_plots(validation_long, 'env', 'geth', 'cost_ns')
compare_plots(validation_long, 'env', 'evmone', 'cost_ns')
compare_plots(validation_long, 'env', 'openethereum', 'cost_ns')
```

## Model diagnostics

```{r fig.width=15}
par(mfrow=c(3,4))
plot(model_geth)
plot(model_evmone)
plot(model_openethereum)
```

### Discussion of results so far

Results for `geth` and `evmone` look promising, the correlation is clearly there, the slope coefficient is close to 1 (so there's a good correspondence between a nanosecond of measurement and the nanosecond of validation).

#### Intercept problem

**NOTE** this has been solved, see last paragrapsh

The worrying part is the intercept coefficient which is far away from zero (should be the cost of an empty EVM program). If we use a validation set with only small arguments (`smallpush`), then the intercept problem goes away, so it seems that it might be coming from the estimates of the arguments impact.

The intercept problem has been explained by randomizing the length of the random program. We can see on the plots of the entire program set, that the relation between estimated cost and actual program execution time is non-linear (the longer the program, the more efficient the execution and the slope coefficient approaches 1.0). At the same time, the shorter the program, the smaller the intercept.

**TODO** increase the upper bound for the program length even further, to cover the area where the slope is steadily 1.0. Also increase the total count of program samples to have a good representation in all lengths.

#### `openethereum` problem

**NOTE** this has been solved, see last paragraph.

`openethereum` has very bad results, the slope coefficient isn't even significant. If we use `smallpush` it becomes significant around 0.55. `marginal` and `individual` measurements actually yield much better validation models (see `individual_vs_total_validation.Rmd` which explores that) than `arguments` measurement. It is likely up to the problems identified during the `arguments` measurement analysis for `openethereum`, namely:
1. The `OPCODE`s which turn out to have significant and impactful arguments are "weird", e.g. `BYTE` is impacted by both it's arguments, `DIV`-like OPCODEs have surprisingly small impact, `XOR` etc.

There's an interesting phenomenon on the `openethereum` validation, if you use `smallpush` - the validation time seems to be very "bimodal" - there's a gap between two clusters of points - one cluster is overestimated and the other cluster is underestimated. Very few points estimate "on point". It is also observable during manual trials of the measurements - some complete much faster than the others.

Another note is that increasing sample size and/or number of samples for the validation set, improves the validation model. It might be that we're brute-forcing over the problem of the unstability of `openethereum`'s performance (see above).

Lastly, increasing the sample size on the estimation set (`marginal/arguments`), improves the validation model further, and also makes the estimates much more reasonable.