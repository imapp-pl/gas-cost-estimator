---
title: "R Notebook: exploration of validation of measurements - with varying arguments; for Gas Cost Estimator"
output: html_document
---

```{r include=FALSE, fig.width=20}
library(sqldf)
library(nlme)

# prevent scientific notation
options(scipen = 100)
```

### Data preparations

```{r}
explained_variable = 'avg_measure_total_time_ns'
all_envs = c('geth', 'evmone')
```

```{r}
plot_validation <- function(df, yvar, xvar, labels, colors, set_asp, log) {
  if (missing(labels)) {
    labels = 'program_id'
  }
  if (missing(colors)) {
    colors = 'has_exp'
  }
  if (missing(set_asp) || set_asp == FALSE) {
    asp=NULL
  } else {
    asp = 1
  }
  if (missing(log)) {
    log = ''
  }
  scatter.smooth(df[, xvar], df[, yvar], col=df[, colors], asp=asp, log=log)
  text(df[, yvar] ~ df[, xvar], data=df, labels=df[, labels], cex=0.6, pos=4, font=2)
}
```

```{r}
lm_validation <- function(df, yvar, xvar, selection_col, selection_val) {
  if (! missing(selection_col) & ! missing(selection_val)) {
    subset = df[which(df[, selection_col] == selection_val), ]
  } else {
    subset = df
  }
  model = lm(subset[, yvar] ~ subset[,xvar], data=subset)
  print(summary(model))
  return(model)
}
```

```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")

estimated_cost = read.csv("../../local/argument_estimated_cost.csv")
# those are only the constant gas costs, we'll calculate arg portion of the cost later
current_gas_cost = read.csv("program_generator/data/current_gas_cost.csv")
# from measure_marginal.Rmd, not currently used except experimentation with mixing (see below)
marginal = read.csv("../../local/marginal_estimated_cost.csv")

# TODO: PUSHes aren't estimated and are prevalent in the validation programs. Rectify this.
```

#### Mix marginal and argument measurements?

We originally used only the results coming from the "arguments" measurements. This means that the programs used featured running the OPCODEs with varying arguments (spanning the entire space of uint256).
For the constant cost of an OPCODE, we'd take the `op_count` variable estimation.
This turned out to give very coarse estimate of the constant cost (clustered estimations for evmone, grossly overestimated constant cost of MOD and friends).

We next tried to "mix" the two results. Take the constant cost estimated via "measure marginal" (simple, constant arguments, varying length of a series of OPCODEs) and take the estimation of the argument impact from the "measure argument" approach.

The drawbacks of the mixed approach are:
  - might be OPCODE-unfair
  - give slightly less healthy results (with the only-"arguments", the slope coefficients are closer to 1)
  - is more complicated
  - after the fixes to the "arguments" measurement routine, as well as switching to the `c100_ops300_clean` validation set (which doesn't limit args to `PUSH1`-size), the only-"arguments" method doesn't give bad results anymore
  
Therefore, in the end, we stick with only-"arguments" measurements, and leave "marginal" measurements for cross validation only.

```{r}
order1 = order(estimated_cost$opcode, estimated_cost$env)
order2 = order(marginal$op, marginal$env)

# commented out - we **don't** mix
estimated_cost[order1, 'estimate_marginal_ns'] = marginal[order2, 'estimate_marginal_ns']

head(estimated_cost)
head(marginal)
```

See `individual_vs_total_validation.Rmd` for invocations to generate/trace/measure programs.

We've extended the program length to 30 OPCODEs to accommodate more `PUSH`es and `POP`s.

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")

# we have tried `c900_ops300_clean` too, but it is a bit overkill. No surprises there.
# program_set_codename = "arythmetic_c100_ops300_clean"
# measurement_codename = "200_8"

# we randomize the number of instructions in the random programs to capture program length impact on OPCODE performance
# program_set_codename = "arythmetic_c100_randOps500_clean"
# measurement_codename = "200_8"

# this dataset (tiny programs) has a radically highler slope coefficient and lower Intercept
# this prompt us to vary the opsLimit for the validation program generation
# program_set_codename = "arythmetic_c100_ops10_clean"
# measurement_codename = "200_8"

# this is the data set without varying argument size. 
# program_set_codename = "arythmetic_c100_ops30_clean_smallpush"
# measurement_codename = "50_1"

program_set_codename = "full5_c100_randOps1500_clean_prejump_randominant"
measurement_codename = "200_8"

programs = read.csv(paste("../../local/pg_validation_", program_set_codename, ".csv", sep=""))

traces = read.csv(paste("../../local/trace_pg_validation_", program_set_codename, ".csv", sep=""))
head(traces)
total_measurements_geth = read.csv(paste("../../local/geth_pg_validation_", program_set_codename, "_", measurement_codename, ".csv", sep=""))
head(total_measurements_geth)
total_measurements_evmone = read.csv(paste("../../local/evmone_pg_validation_", program_set_codename, "_", measurement_codename, ".csv", sep=""))
head(total_measurements_evmone)

total_measurements_geth$env = 'geth'
total_measurements_evmone$env = 'evmone'
measurements = rbind(total_measurements_geth, total_measurements_evmone)
head(measurements)
```

### Remove outliers

```{r fig.width=15}
remove_outliers <- function(df, col) {
  boxplot_result = boxplot(df[, col] ~ df[, 'program_id'] + df[, 'env'], plot=FALSE)
  outliers = boxplot_result$out
  names = boxplot_result$names[boxplot_result$group]
  all_row_identifiers = paste(df[, col], df[, 'program_id'], df[, 'env'], sep='.')
  outlier_row_identifiers = paste(outliers, names, sep='.')
  no_outliers = df[-which(all_row_identifiers %in% outlier_row_identifiers), ]
  return(no_outliers)
}

removed_outliers = TRUE

if (removed_outliers) {
  # before
  par(mfrow=c(2,2))
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'geth'), ], las=2, outline=TRUE, log='y')
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'evmone'), ], las=2, outline=TRUE, log='y')

  measurements = remove_outliers(measurements, 'measure_total_time_ns')
  
  # after
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'geth'), ], las=2, outline=TRUE, log='y')
  boxplot(measure_total_time_ns ~ program_id, data=measurements[which(measurements$env == 'evmone'), ], las=2, outline=TRUE, log='y')
}
```

```{r}
estimated_cost$arg0_ns[which(is.na(estimated_cost$arg0_ns))] = 0
estimated_cost$arg1_ns[which(is.na(estimated_cost$arg1_ns))] = 0
estimated_cost$arg2_ns[which(is.na(estimated_cost$arg2_ns))] = 0

estimated_traces = sqldf("SELECT
                            program_id, sample_id, instruction_id, op,
                            arg_0 as arg0, arg_1 as arg1, arg_2 as arg2,
                            env,
                            arg0_ns, arg1_ns, arg2_ns, estimate_marginal_ns,
                            constant_current_gas
                          FROM traces
                          INNER JOIN estimated_cost ON
                            traces.op == estimated_cost.opcode
                          INNER JOIN current_gas_cost ON
                            traces.op == current_gas_cost.opcode")

# TODO: very crude, how can this be made more robust?

estimated_traces$arg0[which(is.na(estimated_traces$arg0))] = 0
estimated_traces$arg1[which(is.na(estimated_traces$arg1))] = 0
estimated_traces$arg2[which(is.na(estimated_traces$arg2))] = 0

arg0_linear_opcodes = c('CALLDATALOAD', 'CALLDATACOPY', 'RETURNDATACOPY', 'MLOAD', 'MSTORE', 'MSTORE8', 'CODECOPY')
arg1_linear_opcodes = c('CALLDATACOPY', 'RETURNDATACOPY', 'CODECOPY')
arg2_linear_opcodes = c('CALLDATACOPY', 'RETURNDATACOPY', 'CODECOPY')

arg0_estimated_cost_log = estimated_traces$arg0_ns * log(estimated_traces$arg0 + 1, 256) * !(estimated_traces$op %in% arg0_linear_opcodes)
arg1_estimated_cost_log = estimated_traces$arg1_ns * log(estimated_traces$arg1 + 1, 256) * !(estimated_traces$op %in% arg1_linear_opcodes)
arg2_estimated_cost_log = estimated_traces$arg2_ns * log(estimated_traces$arg2 + 1, 256) * !(estimated_traces$op %in% arg2_linear_opcodes)
arg0_estimated_cost_lin = estimated_traces$arg0_ns * estimated_traces$arg0 * (estimated_traces$op %in% arg0_linear_opcodes)
arg1_estimated_cost_lin = estimated_traces$arg1_ns * estimated_traces$arg1 * (estimated_traces$op %in% arg1_linear_opcodes)
arg2_estimated_cost_lin = estimated_traces$arg2_ns * estimated_traces$arg2 * (estimated_traces$op %in% arg2_linear_opcodes)
arg0_estimated_cost = arg0_estimated_cost_log + arg0_estimated_cost_lin
arg1_estimated_cost = arg1_estimated_cost_log + arg1_estimated_cost_lin
arg2_estimated_cost = arg2_estimated_cost_log + arg2_estimated_cost_lin
estimated_traces$cost_ns = estimated_traces$estimate_marginal_ns + 
                           arg0_estimated_cost +
                           arg1_estimated_cost +
                           arg2_estimated_cost
```

```{r}
arg1_current_gas = (estimated_traces$arg1 != 0) * 50 * (log(estimated_traces$arg1, 256) + 1) * (estimated_traces$op == 'EXP')
arg2_current_gas = 3 * ceiling(estimated_traces$arg2 / 32) * (estimated_traces$op %in% arg2_linear_opcodes)
estimated_traces$current_gas = estimated_traces$constant_current_gas +
                               arg1_current_gas +
                               arg2_current_gas
```

```{r}
estimated_programs = sqldf("SELECT
                              estimated_traces.program_id,
                              sum(cost_ns) as cost_ns,
                              sum(current_gas) as current_gas,
                              env,
                              dominant
                           FROM estimated_traces
                           INNER JOIN programs ON
                                 programs.program_id == estimated_traces.program_id
                           GROUP BY estimated_traces.program_id, env, dominant")

aggregate_measurements = sqldf("SELECT
                                  program_id,
                                  env,
                                  avg(measure_total_time_ns) as avg_measure_total_time_ns,
                                  min(measure_total_time_ns) as min_measure_total_time_ns
                                FROM measurements
                                GROUP BY env, program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg_measure_total_time_ns,
                      min_measure_total_time_ns,
                      cost_ns,
                      current_gas,
                      estimated_programs.env,
                      dominant
                    FROM estimated_programs
                    INNER JOIN aggregate_measurements ON 
                          aggregate_measurements.program_id == estimated_programs.program_id
                      AND aggregate_measurements.env == estimated_programs.env
                    ORDER BY estimated_programs.env, cast(estimated_programs.program_id AS int)")

head(validation)
```

We need an additional simple (plottable) "fingerprint" of the OPCODEs included in the program, to evaluate the validation plot at a glance.

```{r}
fingerprint <- function(x) {
  excluded = x$op %in% c('PUSH4', 'PUSH1', 'MSTORE8', 'STOP', 'POP', 'JUMP', 'JUMPDEST')
  opcodes = unique(x[which(!excluded),'op'])
  paste(opcodes, collapse=' ')
}
fingerprint_no_push <- function(x) {
  excluded = x$op %in% c('PUSH1', 'PUSH2', 'PUSH3', 'PUSH4', 'PUSH5', 'PUSH6', 'PUSH7', 'PUSH8', 'PUSH9', 'PUSH10', 'PUSH11', 'PUSH12', 'PUSH13', 'PUSH14', 'PUSH15', 'PUSH16', 'PUSH17', 'PUSH18', 'PUSH19', 'PUSH20', 'PUSH21', 'PUSH22', 'PUSH23', 'PUSH24', 'PUSH25', 'PUSH26', 'PUSH27', 'PUSH28', 'PUSH29', 'PUSH30', 'PUSH31', 'PUSH32', 'MSTORE8', 'STOP', 'POP', 'JUMP', 'JUMPDEST')
  opcodes = unique(x[which(!excluded),'op'])
  paste(opcodes, collapse=' ')
}
has_exp <- function(x) {
  if ('EXP' %in% x$op) 'red' else 'blue'
}
has_jump <- function(x) {
  if ('JUMP' %in% x$op | 'JUMPI' %in% x$op) 'red' else 'blue'
}
count_zero_args <- function(x) {
  sum(!is.na(x$arg_0) & x$arg_0 == 0 | !is.na(x$arg_1) & x$arg_1 == 0 | !is.na(x$arg_2) & x$arg_2 == 0)
}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$fingerprint_no_push = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint_no_push(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$has_jump = gapply(traces, c('op'), FUN=function(x) has_jump(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'arg_0', 'arg_1', 'arg_2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

```{r fig.width=15}
# we've removed outliers so picking the right ylim / OUTLINE no longer required
par(mfrow=c(2,2))
overview_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  boxplot(measure_total_time_ns ~ program_id, data=subset, main=selection_val)
  boxplot(measure_total_time_ns ~ run_id, data=subset, las=2, main=selection_val)
}
overview_plots(measurements, 'env', 'geth')
overview_plots(measurements, 'env', 'evmone')
```

### Check estimation using trivial variables

Based on the plots alone, it does not appear like our estimation only relies on trivial variables.

```{r}
program_length <- function(x) {
  max(x$instruction_id)
}
# the "trivial" variables to check if we are not doing trivial estimation
validation$program_length = gapply(traces, c('program_id', 'instruction_id'), FUN=function(x) program_length(x), groups=traces$program_id)
```

We divide the validation data into classes with respect to the length of the program:

```{r}
validation_long = validation[which(validation$program_length > 400), ]
validation_short = validation[which(validation$program_length <= 100), ]
validation_no_exp = validation[which(validation$dominant != 'EXP'), ]
```

```{r fig.width=15}
trivial_variable_plots <- function(df, selection_col, selection_val, labels, colors, log) {
  if (missing(labels)) {
    labels = 'program_id'
  }
  if (missing(colors)) {
    colors = 'has_exp'
  }
  if (missing(log)) {
    log = ''
  }
  subset = df[which(df[, selection_col] == selection_val), ]
  plot_validation(subset, explained_variable, 'program_length', labels=labels, colors=colors, log=log)
  title(main=paste(selection_val, 'program_length'))
}
```
Trivial variable plots are interesting, after we have introduced varying program length. `openethereum` in particular has a good correlation there, which is caused by the large, constant cost of running an instruction (~250ns, regardless of OPCODE).

We add the 'no EXP' version to make sure that the improvement of the estimated gas model over program length model isn't only because of the extreme EXP case.

```{r}
print('geth all - only progam length')
lm_validation(validation, explained_variable, 'program_length', selection_col='env', selection_val='geth')
print('evmone all - only progam length')
lm_validation(validation, explained_variable, 'program_length', selection_col='env', selection_val='evmone')
print('geth all - only progam length - no EXP')
lm_validation(validation_no_exp, explained_variable, 'program_length', selection_col='env', selection_val='geth')
print('evmone all - only progam length  - no EXP')
lm_validation(validation_no_exp, explained_variable, 'program_length', selection_col='env', selection_val='evmone')
```

Nevertheless, neither of the "trivial" models is comparable with the final validation model including the estimated `cost_ns`. see below.

### Current gas cost models

```{r}
print('geth all - current gas')
lm_validation(validation, explained_variable, 'current_gas', selection_col='env', selection_val='geth')
print('evmone all - current gas')
lm_validation(validation, explained_variable, 'current_gas', selection_col='env', selection_val='evmone')
```

### Final validation model

We explore the models in two classes of program lengths. We keep the model for the long programs.

```{r}
compare_plots <- function(df, selection_col, selection_val, estimate_var, labels, colors, log) {
  if (missing(labels)) {
    labels = 'program_id'
  }
  if (missing(colors)) {
    colors = 'has_exp'
  }
  if (missing(log)) {
    log = ''
  }
  subset = df[which(df[, selection_col] == selection_val), ]
  # TODO: set_asp=TRUE works better when models are sane
  plot_validation(subset, explained_variable, estimate_var, labels=labels, colors=colors, log=log) #, set_asp=TRUE)
  title(main=paste(selection_val, ' estimate'))
}

print('geth short')
lm_validation(validation_short, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
print('evmone short')
lm_validation(validation_short, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')

print('geth long')
lm_validation(validation_long, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
print('evmone long')
lm_validation(validation_long, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')

print('geth all - no EXP')
lm_validation(validation_no_exp, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
print('evmone all - no EXP')
lm_validation(validation_no_exp, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')

print('geth all')
model_geth = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
print('evmone all')
model_evmone = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')
```

```{r fig.width=15}
par(mfrow=c(4,2))
compare_plots(validation_short, 'env', 'geth', 'cost_ns')
compare_plots(validation_short, 'env', 'evmone', 'cost_ns')
compare_plots(validation_long, 'env', 'geth', 'cost_ns')
compare_plots(validation_long, 'env', 'evmone', 'cost_ns')
compare_plots(validation_no_exp, 'env', 'geth', 'cost_ns')
compare_plots(validation_no_exp, 'env', 'evmone', 'cost_ns')
compare_plots(validation, 'env', 'geth', 'cost_ns')
compare_plots(validation, 'env', 'evmone', 'cost_ns')
```

A side-by-side comparison of the gas cost model with the `program_length` model.

```{r fig.width=25}
par(mfrow=c(2,2))
trivial_variable_plots(validation_no_exp, 'env', 'geth', labels='dominant')
trivial_variable_plots(validation_no_exp, 'env', 'evmone', labels='dominant')
compare_plots(validation_no_exp, 'env', 'geth', 'cost_ns', labels='dominant')
compare_plots(validation_no_exp, 'env', 'evmone', 'cost_ns', labels='dominant')
par(mfrow=c(2,2))
trivial_variable_plots(validation, 'env', 'geth', labels='dominant', log='xy')
trivial_variable_plots(validation, 'env', 'evmone', labels='dominant', log='xy')
compare_plots(validation, 'env', 'geth', 'cost_ns', labels='dominant', log='xy')
compare_plots(validation, 'env', 'evmone', 'cost_ns', labels='dominant', log='xy')
par(mfrow=c(2,2))
trivial_variable_plots(validation, 'env', 'geth', labels='dominant')
trivial_variable_plots(validation, 'env', 'evmone', labels='dominant')
compare_plots(validation, 'env', 'geth', 'cost_ns', labels='dominant')
compare_plots(validation, 'env', 'evmone', 'cost_ns', labels='dominant')
```

A side-by-side comparison of the gas cost model with the `current_gas` model.

```{r fig.width=25}
par(mfrow=c(2,2))
compare_plots(validation_no_exp, 'env', 'geth', 'current_gas', labels='dominant')
compare_plots(validation_no_exp, 'env', 'evmone', 'current_gas', labels='dominant')
compare_plots(validation_no_exp, 'env', 'geth', 'cost_ns', labels='dominant')
compare_plots(validation_no_exp, 'env', 'evmone', 'cost_ns', labels='dominant')
par(mfrow=c(2,2))
compare_plots(validation, 'env', 'geth', 'current_gas', labels='dominant', log='xy')
compare_plots(validation, 'env', 'evmone', 'current_gas', labels='dominant', log='xy')
compare_plots(validation, 'env', 'geth', 'cost_ns', labels='dominant', log='xy')
compare_plots(validation, 'env', 'evmone', 'cost_ns', labels='dominant', log='xy')
par(mfrow=c(2,2))
compare_plots(validation, 'env', 'geth', 'current_gas', labels='dominant')
compare_plots(validation, 'env', 'evmone', 'current_gas', labels='dominant')
compare_plots(validation, 'env', 'geth', 'cost_ns', labels='dominant')
compare_plots(validation, 'env', 'evmone', 'cost_ns', labels='dominant')
```

## Model diagnostics

```{r fig.width=15}
par(mfrow=c(2,4))
plot(model_geth)
plot(model_evmone)
```

### Discussion of results so far

Results for `geth` and `evmone` look promising, the correlation is clearly there, the slope coefficient is close to 1 (so there's a good correspondence between a nanosecond of measurement and the nanosecond of validation).

The model diagnosticts aren't ideal, especially for `geth`, where strong non-linearity is present because of the overestimated EXP programs.

The best trait of the model is that it offers a massive improvement over the trivial model.

#### Intercept problem

**NOTE** this has been solved, see last paragrapsh

The worrying part is the intercept coefficient which is far away from zero (should be the cost of an empty EVM program). If we use a validation set with only small arguments (`smallpush`), then the intercept problem goes away, so it seems that it might be coming from the estimates of the arguments impact.

The intercept problem has been explained by randomizing the length of the random program. We can see on the plots of the entire program set, that the relation between estimated cost and actual program execution time is non-linear (the longer the program, the more efficient the execution and the slope coefficient approaches 1.0). At the same time, the shorter the program, the smaller the intercept.

**NOTE** After including memory OPCODEs the intercept is back, because we're allocating a considerable chunk of memory at start of every program.

#### `openethereum` problem

**NOTE** this has been solved, see last paragraph.

`openethereum` has very bad results, the slope coefficient isn't even significant. If we use `smallpush` it becomes significant around 0.55. `marginal` and `individual` measurements actually yield much better validation models (see `individual_vs_total_validation.Rmd` which explores that) than `arguments` measurement. It is likely up to the problems identified during the `arguments` measurement analysis for `openethereum`, namely:
1. The `OPCODE`s which turn out to have significant and impactful arguments are "weird", e.g. `BYTE` is impacted by both it's arguments, `DIV`-like OPCODEs have surprisingly small impact, `XOR` etc.

There's an interesting phenomenon on the `openethereum` validation, if you use `smallpush` - the validation time seems to be very "bimodal" - there's a gap between two clusters of points - one cluster is overestimated and the other cluster is underestimated. Very few points estimate "on point". It is also observable during manual trials of the measurements - some complete much faster than the others.

Another note is that increasing sample size and/or number of samples for the validation set, improves the validation model. It might be that we're brute-forcing over the problem of the unstability of `openethereum`'s performance (see above).

Lastly, increasing the sample size on the estimation set (`marginal/arguments`), improves the validation model further, and also makes the estimates much more reasonable.