---
title: "R Notebook: exploration of validation of measurements - with varying arguments; for Gas Cost Estimator"
output: html_notebook
---

```{r include=FALSE, fig.width=20}
library(sqldf)
library(nlme)

# prevent scientific notation
options(scipen = 100)
```

### Data preparations

We need an additional simple (plottable) "fingerprint" of the OPCODEs included in the program, to evaluate the validation plot at a glance.

```{r}
fingerprint <- function(x) {
  paste(x[which(!(x$op %in% c('PUSH32', 'PUSH1', 'STOP', 'POP'))),'op'], collapse=' ')
}
has_exp <- function(x) {
  if ('EXP' %in% x$op) 'red' else 'blue'
}
count_zero_args <- function(x) {
  sum(!is.na(x$arg_0) & x$arg_0 == 0 | !is.na(x$arg_1) & x$arg_1 == 0 | !is.na(x$arg_2) & x$arg_2 == 0)
}
```

```{r}
explained_variable = 'avg_measure_total_time_ns'
all_envs = c('geth', 'evmone', 'openethereum')
```

```{r}
plot_validation <- function(df, yvar, xvar, label_n_zero_args) {
  if (missing(label_n_zero_args)) {
    label_n_zero_args = FALSE
  }
  scatter.smooth(df[,xvar], df[,yvar], col=df$has_exp)
  text(df[,yvar] ~ df[,xvar], data=df, labels=program_id, cex=0.6, pos=4, font=2)
  if (label_n_zero_args) {
    text(df[,yvar] ~ df[,xvar], data=df, labels=n_zero_args, cex=0.6, pos=1, font=2)
  }
}
```

```{r}
lm_validation <- function(df, yvar, xvar, selection_col, selection_val) {
  if (! missing(selection_col) & ! missing(selection_val)) {
    subset = df[which(df[, selection_col] == selection_val), ]
  } else {
    subset = df
  }
  lm(subset[, yvar] ~ subset[,xvar], data=subset)
}
```

```{r}
setwd("~/sources/imapp/gas-cost-estimator/src")

estimated_cost = read.csv("../../local/argument_estimated_cost.csv")
# from measure_marginal.Rmd
# TODO we need this to fill in missing PUSHes and POP, to be rectified later
marginal = read.csv("../../local/marginal_estimated_cost.csv")

# TODO remove when rectified
for (opcode in c('PUSH1', 'POP')) {
  for (env in all_envs) {
    # working around the ridiculous type coercion of `c()`
    estimated_cost[nrow(estimated_cost) + 1, 1:2] = c(opcode, env)
    estimated_cost[nrow(estimated_cost), 3:4] = c(FALSE, FALSE)
    estimated_cost[nrow(estimated_cost), 5:8] = c(marginal[which(marginal$op==opcode & marginal$env==env), 'estimate_marginal_ns'], NA, NA, NA)
  }
}
```

#### Mix marginal and argument measurements?

We originally used only the results coming from the "arguments" measurements. This means that the programs used featured running the OPCODEs with varying arguments (spanning the entire space of uint256).
For the constant cost of an OPCODE, we'd take the `op_count` variable estimation.
This turned out to give very coarse estimate of the constant cost (clustered estimations for evmone, grossly overestimated constant cost of MOD and friends).

We next tried to "mix" the two results. Take the constant cost estimated via "measure marginal" (simple, constant arguments, varying length of a series of OPCODEs) and take the estimation of the argument impact from the "measure argument" approach.

The drawbacks of the mixed approach are:
  - might be OPCODE-unfair
  - give slightly less healthy results (with the only-"arguments", the slope coefficients are closer to 1)
  - is more complicated
  - after the fixes to the "arguments" measurement routine, as well as switching to the `c100_ops300_clean` validation set (which doesn't limit args to `PUSH1`-size), the only-"arguments" method doesn't give bad results anymore
  
Therefore, in the end, we stick with only-"arguments" measurements, and leave "marginal" measurements for cross validation only.

```{r}
order1 = order(estimated_cost$opcode, estimated_cost$env)
order2 = order(marginal$op, marginal$env)

# commented out - we **don't** mix
# estimated_cost[order1, 'estimate_marginal_ns'] = marginal[order2, 'estimate_marginal_ns']

head(estimated_cost)
head(marginal)
```

See `individual_vs_total_validation.Rmd` for invocations to generate/trace/measure programs.

We've extended the program length to 30 OPCODEs to accommodate more `PUSH`es and `POP`s.

```{r fig.width=20}
setwd("~/sources/imapp/gas-cost-estimator/src")

# we have tried `c900_ops300_clean` too, but it is a bit overkill. No surprises there.
program_set_codename = "c100_ops300_clean"
measurement_codename = "200_8"

# this is the data set without varying argument size. 
# program_set_codename = "c100_ops30_clean_smallpush"
# measurement_codename = "50_1"

traces = read.csv(paste("../../local/trace_pg_arythmetic_", program_set_codename, ".csv", sep=""))
head(traces)
total_measurements_geth = read.csv(paste("../../local/geth_pg_arythmetic_", program_set_codename, "_", measurement_codename, ".csv", sep=""))
head(total_measurements_geth)
total_measurements_evmone = read.csv(paste("../../local/evmone_pg_arythmetic_", program_set_codename, "_", measurement_codename, ".csv", sep=""))
head(total_measurements_evmone)
# TODO: at this point, for openethereum we are getting much better (acceptable) results for the sample size 16, nsamples 400, see Discussion section below
total_measurements_openethereum = read.csv(paste("../../local/openethereum_pg_arythmetic_", program_set_codename, "_", "16_400", ".csv", sep=""))
head(total_measurements_openethereum)
```

### Remove outliers

```{r}
remove_outliers <- function(df, col) {
  # we don't have subsets, so the entire df is a subset
  subset = df
  outliers = boxplot(subset[, col], plot=FALSE)$out
  # NOTE here we're also not filtering by the subset
  no_outliers = df[-which(df[, col] %in% outliers), ]
  return(no_outliers)
}

total_measurements_geth = remove_outliers(total_measurements_geth, "measure_total_time_ns")
head(total_measurements_geth)
total_measurements_evmone = remove_outliers(total_measurements_evmone, "measure_total_time_ns")
head(total_measurements_evmone)
total_measurements_openethereum = remove_outliers(total_measurements_openethereum, "measure_total_time_ns")
head(total_measurements_openethereum)
```

```{r}
total_measurements_geth$env = 'geth'
total_measurements_evmone$env = 'evmone'
total_measurements_openethereum$env = 'openethereum'
total_measurements = rbind(total_measurements_geth, total_measurements_evmone, total_measurements_openethereum)
head(total_measurements)
```

```{r}
estimated_cost$arg0_ns[which(is.na(estimated_cost$arg0_ns))] = 0
estimated_cost$arg1_ns[which(is.na(estimated_cost$arg1_ns))] = 0
estimated_cost$arg2_ns[which(is.na(estimated_cost$arg2_ns))] = 0

estimated_traces = sqldf("SELECT
                            program_id, sample_id, instruction_id, op,
                            arg_0 as arg0, arg_1 as arg1, arg_2 as arg2,
                            env,
                            arg0_ns, arg1_ns, arg2_ns, estimate_marginal_ns
                          FROM traces
                          INNER JOIN estimated_cost ON
                            traces.op == estimated_cost.opcode")

# TODO: very crude, how can this be made more robust?

estimated_traces$arg0[which(is.na(estimated_traces$arg0))] = 0
estimated_traces$arg1[which(is.na(estimated_traces$arg1))] = 0
estimated_traces$arg2[which(is.na(estimated_traces$arg2))] = 0

estimated_traces$cost_ns = estimated_traces$estimate_marginal_ns + 
                           estimated_traces$arg0_ns * log(estimated_traces$arg0 + 1, 256) +
                           estimated_traces$arg1_ns * log(estimated_traces$arg1 + 1, 256) +
                           estimated_traces$arg2_ns * log(estimated_traces$arg2 + 1, 256)
```

```{r}
estimated_programs = sqldf("SELECT
                              program_id,
                              sum(cost_ns) as cost_ns,
                              env
                           FROM estimated_traces
                           GROUP BY program_id, env")

aggregate_measurements = sqldf("SELECT
                                  program_id,
                                  env,
                                  avg(measure_total_time_ns) as avg_measure_total_time_ns,
                                  min(measure_total_time_ns) as min_measure_total_time_ns
                                FROM total_measurements
                                GROUP BY env, program_id")

# ORDER BY is needed here, so that the programs are ordered by program_id numerically
# this is a hacky way of ensuring we can merge it with other program data like fingerprints
# TODO: make this more robust
validation = sqldf("SELECT
                      estimated_programs.program_id,
                      avg_measure_total_time_ns,
                      min_measure_total_time_ns,
                      cost_ns,
                      estimated_programs.env
                    FROM estimated_programs
                    INNER JOIN aggregate_measurements ON 
                          aggregate_measurements.program_id == estimated_programs.program_id
                      AND aggregate_measurements.env == estimated_programs.env
                    ORDER BY estimated_programs.env, cast(estimated_programs.program_id AS int)")

head(validation)
```

```{r}
validation$fingerprint = gapply(traces, c('op', 'program_id'), FUN=function(x) fingerprint(x), groups=traces$program_id)
validation$has_exp = gapply(traces, c('op'), FUN=function(x) has_exp(x), groups=traces$program_id)
validation$n_zero_args = gapply(traces, c('op', 'program_id', 'arg_0', 'arg_1', 'arg_2'), FUN=function(x) count_zero_args(x), groups=traces$program_id)
```

```{r fig.width=15}
# we've removed outliers so picking the right ylim / OUTLINE no longer required

overview_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  plot(subset$measure_total_time_ns, main=selection_val)
  boxplot(measure_total_time_ns ~ program_id, data=subset, main=selection_val)
  boxplot(measure_total_time_ns ~ run_id, data=subset, las=2, main=selection_val)
}
overview_plots(total_measurements, 'env', 'geth')
overview_plots(total_measurements, 'env', 'evmone')
overview_plots(total_measurements, 'env', 'openethereum')
```

### Check estimation using trivial variables

Based on the plots alone, it does not appear like our estimation only relies on trivial variables.

```{r}
program_length <- function(x) {
  max(x$instruction_id)
}
count_op <- function(x, op) {
  sum(x$op == op)
}
# the "trivial" variables to check if we are not doing trivial estimation
validation$program_length = gapply(traces, c('program_id', 'instruction_id'), FUN=function(x) program_length(x), groups=traces$program_id)
validation$n_push = gapply(traces, c('program_id', 'op'), FUN=function(x) count_op(x, 'PUSH1'), groups=traces$program_id)
validation$n_pop = gapply(traces, c('program_id', 'op'), FUN=function(x) count_op(x, 'POP'), groups=traces$program_id)
```

```{r}
trivial_variable_plots <- function(df, selection_col, selection_val) {
  subset = df[which(df[, selection_col] == selection_val), ]
  par(mfrow=c(1,3))
  plot_validation(subset, explained_variable, 'program_length')
  title(main=selection_val)
  plot_validation(subset, explained_variable, 'n_push')
  title(main=selection_val)
  plot_validation(subset, explained_variable, 'n_pop')
  title(main=selection_val)
}
trivial_variable_plots(validation, 'env', 'geth')
trivial_variable_plots(validation, 'env', 'evmone')
trivial_variable_plots(validation, 'env', 'openethereum')
```
### Final validation model

```{r}
compare_plots <- function(df, selection_col, selection_val, estimate_var) {
  subset = df[which(df[, selection_col] == selection_val), ]
  plot_validation(subset, explained_variable, estimate_var)
  title(main=paste(selection_val, ' estimate'))
}

model_geth = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='geth')
model_evmone = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='evmone')
model_openethereum = lm_validation(validation, explained_variable, 'cost_ns', selection_col='env', selection_val='openethereum')
print('geth')
summary(model_geth)
print('evmone')
summary(model_evmone)
print('openethereum')
summary(model_openethereum)
```

```{r}
compare_plots(validation, 'env', 'geth', 'cost_ns')
compare_plots(validation, 'env', 'evmone', 'cost_ns')
compare_plots(validation, 'env', 'openethereum', 'cost_ns')
```

## Model diagnostics



```{r fig.width=15}
par(mfrow=c(3,4))
plot(model_geth)
plot(model_evmone)
plot(model_openethereum)
```

### Discussion of results so far

Results for `geth` and `evmone` look promising, the correlation is clearly there, the slope coefficient is very close to 1 (so there's a good correspondence between a nanosecond of measurement and the nanosecond of validation).

The worrying part is the intercept coefficient which is far away from zero (should be the cost of an empty EVM program). **NOTE** if we use a validation set with only small arguments (`smallpush`), then the intercept problem goes away, so it seems that it might be coming from the estimates of the arguments impact.

#### `openethereum` problem

**NOTE** this has been solved, see last paragraph.

`openethereum` has very bad results, the slope coefficient isn't even significant. If we use `smallpush` it becomes significant around 0.55. `marginal` and `individual` measurements actually yield much better validation models (see `individual_vs_total_validation.Rmd` which explores that) than `arguments` measurement. It is likely up to the problems identified during the `arguments` measurement analysis for `openethereum`, namely:
1. The `OPCODE`s which turn out to have significant and impactful arguments are "weird", e.g. `BYTE` is impacted by both it's arguments, `DIV`-like OPCODEs have surprisingly small impact, `XOR` etc.

There's an interesting phenomenon on the `openethereum` validation, if you use `smallpush` - the validation time seems to be very "bimodal" - there's a gap between two clusters of points - one cluster is overestimated and the other cluster is underestimated. Very few points estimate "on point". It is also observable during manual trials of the measurements - some complete much faster than the others.

Another note is that increasing sample size and/or number of samples for the validation set, improves the validation model. It might be that we're brute-forcing over the problem of the unstability of `openethereum`'s performance (see above).

Lastly, increasing the sample size on the estimation set (`marginal/arguments`), improves the validation model further, and also makes the estimates much "less weird". Only `SHR` remains weird.